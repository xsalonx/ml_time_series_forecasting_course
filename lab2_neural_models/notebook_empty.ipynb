{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a3df5a-c1d1-4761-a7b5-d5dd6299dd65",
   "metadata": {},
   "source": [
    "# Neural forecasting models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7117d4f-abdb-4293-98ec-7a20b7124974",
   "metadata": {},
   "source": [
    "We will cover neural models for time series forecasting, both trained from scratch and pretrained. We will use varied libraries, depending on the model, for example:\n",
    "- [sktime](https://www.sktime.net/en/stable/index.html) - general time series processing\n",
    "- [neuralforecast](https://github.com/Nixtla/neuralforecast) - a lot of neural models for time series, e.g. DLinear, N-BEATS\n",
    "- [PyTorch](https://pytorch.org/) - deep learning framework\n",
    "- [timesfm](https://github.com/google-research/timesfm) - official TimesFM implementation (and loading pretrained model)\n",
    "\n",
    "Use tutorials, quickstarts, GitHub pages etc. of those libraries as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f46ad-7ea2-4135-b5e1-4d1664890295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5963af-1cdb-4999-a8ce-34aa6a414cbd",
   "metadata": {},
   "source": [
    "## Datasets and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb95b0b-43a2-4a91-a092-45f174041296",
   "metadata": {},
   "source": [
    "We will use 2 datasets:\n",
    "1. [Italian pasta dataset](https://www.sciencedirect.com/science/article/abs/pii/S0957417421005431?via%3Dihub), same as in the first notebook.\n",
    "2. [Polish energy production data](https://energy.instrat.pl/en/electrical-system/electricity-production-are/), as published by Energy Instrat and ENTSO-e, from data by PSE (Polskie Sieci Elektroenergetyczne).\n",
    "\n",
    "Both are multivariate and focused on long-term forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e32b2f-6f3f-4ed9-b569-4285c387a553",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Italian pasta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11561af-834c-4365-8bf1-5b1d7d0d2bc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Data loading and visualization\n",
    "\n",
    "This dataset technically multivariate, but it has data from 4 different companies with very different characteristics, so it may have pretty weak cross-series dependencies. We will consider a simplified variant with no exogenous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc668b37-46da-42c1-a223-c9a7f5fbb254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "\n",
    "df_pasta = pd.read_csv(\"italian_pasta.csv\")\n",
    "for num in [1, 2, 3, 4]:\n",
    "    df_pasta[f\"value_B{num}\"] = df_pasta.filter(regex=f\"QTY_B{num}*\").sum(axis=\"columns\")\n",
    "\n",
    "df_pasta = df_pasta.set_index(pd.to_datetime(df_pasta[\"DATE\"])).asfreq(\"d\")\n",
    "df_pasta = df_pasta[[\"value_B1\", \"value_B2\", \"value_B3\", \"value_B4\"]]\n",
    "df_pasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71e12f-f1de-4953-8e03-a3800b6b06c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num in [1, 2, 3, 4]:\n",
    "    plot_series(df_pasta[f\"value_B{num}\"], title=f\"Pasta sales, business {num}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381a4102-30db-4afa-9491-389f01f22916",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "Similarly to the first notebook, we will be interested in long-term forecasting, predicting the daily sales for 2018, based on previous years. Since we have 4 time series with different scales, MASE is a great metric, since it can be averaged across series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63cbfd-2fe8-40a6-8d14-5af2e3d45bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "\n",
    "df_pasta_train = df_pasta[df_pasta.index < \"2018-01-01\"]\n",
    "df_pasta_test = df_pasta[df_pasta.index >= \"2018-01-01\"]\n",
    "\n",
    "imputer = Imputer(method=\"ffill\")\n",
    "df_pasta_train = imputer.fit_transform(df_pasta_train)\n",
    "df_pasta_test = imputer.transform(df_pasta_test)\n",
    "\n",
    "print(f\"Data size: train {len(df_pasta_train)}, test {len(df_pasta_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9582771-e343-4ad5-b8fd-612ae29d7a37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Polish energy production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf64d1f-8a6a-4eeb-9610-f7dfeeeb13ec",
   "metadata": {},
   "source": [
    "#### Data loading and visualization\n",
    "\n",
    "Energy mix is composed of multiple energy sources. It typically consists of multiple components:\n",
    "- slow-changing base, e.g. coal, nuclear\n",
    "- faster changing and controllable sources, e.g. gas, oil, hydro\n",
    "- very cheap, but uncontrollably changing renewables, e.g. wind, solar\n",
    "\n",
    "The resulting production is always limited by the grid efficiency, which is very low in Poland, resulting in e.g. refusing to connect more prosumer solar installations. As such, the production limits are monitored and controlled, and cross-series dependencies are often quite strong.\n",
    "\n",
    "We will aggregate the energy sources a bit, and consider:\n",
    "- coal (and derivatives)\n",
    "- hydro (from all sources)\n",
    "- solar\n",
    "- wind\n",
    "- all others, e.g. oil (petroleum), biomass\n",
    "\n",
    "Since units are GWh (10^9 Wh, Whatt hours), values are very high, so we will consider thousands of GWh, i.e. TWh (10^12 Wh). It is not a standard unit, but should help with numerical stability for methods that do not perform standardization or scaling.\n",
    "\n",
    "Data from PSE has changed its format and processing at 13.06.2024, and values since this date are in 15-minutes intervals, compared to 1-hour from before. As such, we divide them by 4, to have the same unit.\n",
    "\n",
    "If you want to know more about energy production and demand, see e.g. [this video](https://www.youtube.com/watch?v=xhxo2oXRiio) or [this video](https://www.youtube.com/watch?v=GBp_NgrrtPM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e193357-c7ef-4062-8ac3-d69021179032",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_energy = pd.read_csv(\"electricity_production_entsoe_all.csv\")\n",
    "df_energy = df_energy.drop(columns=\"date_utc\")\n",
    "df_energy[\"date\"] = pd.to_datetime(df_energy[\"date\"], format=\"%d.%m.%Y %H:%M\")\n",
    "df_energy = df_energy.set_index(\"date\")\n",
    "df_energy = df_energy.resample(\"D\").sum()\n",
    "\n",
    "# aggregate energy sources\n",
    "df_energy[\"coal\"] = (\n",
    "    df_energy[\"hard_coal\"] + df_energy[\"coal-derived\"] + df_energy[\"lignite\"]\n",
    ")\n",
    "df_energy[\"hydro\"] = (\n",
    "    df_energy[\"hydro_pumped_storage\"] + \n",
    "    df_energy[\"hydro_run-of-river_and_poundage\"] + \n",
    "    df_energy[\"hydro_water_reservoir\"]\n",
    ")\n",
    "df_energy[\"wind\"] = df_energy[\"wind_onshore\"]\n",
    "df_energy[\"other\"] = (\n",
    "    df_energy[\"oil\"] + \n",
    "    df_energy[\"biomass\"] + \n",
    "    df_energy[\"other\"] + \n",
    "    df_energy[\"other_renewable\"]\n",
    ")\n",
    "df_energy = df_energy[[\"coal\", \"gas\", \"hydro\", \"wind\", \"solar\", \"other\"]]\n",
    "\n",
    "# fix values and change units (GWh -> thousands of GWh)\n",
    "df_energy[df_energy.index >= \"13.06.2024\"] /= 4\n",
    "df_energy[\"other\"][df_energy.index >= \"13.06.2024\"] /= 2\n",
    "df_energy = df_energy / 1000\n",
    "\n",
    "df_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd4e2b-78b8-462f-be47-554cdd3ad48b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_series(df_energy.sum(axis=\"columns\"), title=f\"Total energy production\")\n",
    "\n",
    "for col in df_energy.columns:\n",
    "    plot_series(df_energy[col], title=f\"{col.capitalize()} energy production\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdcc2a0-ad9e-4124-8bec-6a0195c6c07e",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "We will perform long-term forecasting, which is a common task on energy production and demand datasets. We will predict production for 2024, using MASE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525b041-1915-4cc0-bab9-55fa612b1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "\n",
    "df_energy_train = df_energy[df_energy.index < \"2024-01-01\"]\n",
    "df_energy_test = df_energy[df_energy.index >= \"2024-01-01\"]\n",
    "\n",
    "print(f\"Data size: train {len(df_energy_train)}, test {len(df_energy_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99585bab-4ef9-4e38-9826-b154201b5e8c",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1eb62f-49a2-4e75-b784-48b9cb95fe71",
   "metadata": {},
   "source": [
    "The sub-sections are independent, and can be implemented in any order. The more you do, the more points (and hence the higher mark) you get. They are also more freeform than previous notebook, and there are more options to choose from.\n",
    "\n",
    "When tuning hyperparameters, choose any strategy you think is reasonable, taking into consideration computational cost and model complexity. Temporal train-valid-test split, time split CV, expanding window - your choice. Even manual tuning is ok, if you think it makes sense, but remember to use the validation set.\n",
    "\n",
    "You can use any framework and tool you want, but suggestions are provided in further sections. Install additional dependencies as needed, either using Poetry and recreating `poetry lock`, or by directly using `!pip install ...`.\n",
    "\n",
    "Training and evaluating more models from particular category can get you more points, as described below. If you prefer, you can also experiment with other models, e.g. RNNs, CNN-based, or state-space models (SSMs), adding further sections. Each one by default is worth 2 points.\n",
    "\n",
    "**Warning:** when making this notebook, some errors with `neuralforecast` cropped up when horizon was greater than 292 for Italian pasta dataset. You can cut the test set at 292 if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c940ad-8e84-4ad1-9853-9b66e70ccaf0",
   "metadata": {},
   "source": [
    "Note that some frameworks (e.g. `neuralforecast`) require \"tall\"/\"long\" time series representation, with columns: `unique_id` (time series identifier), `ds` (date) and `y` (actual value). This is in contrast to the \"wide\" representation, where we have individual series in separate columns, each row with separate date, and values in cells. See e.g. [neuralforecast quickstart](https://nixtlaverse.nixtla.io/neuralforecast/docs/getting-started/quickstart.html#2-loading-airpassengers-data) for an example. Functions prepared below may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cca2b1-a550-4f3a-90e3-3716e469b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def wide_to_long_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = pd.melt(df, ignore_index=False).reset_index(names=\"date\")\n",
    "    df = df.rename(columns={\"variable\": \"unique_id\", \"date\": \"ds\", \"value\": \"y\"})\n",
    "    return df\n",
    "\n",
    "\n",
    "def long_to_wide_df(df: pd.DataFrame, values_col: Optional[str] = None) -> pd.DataFrame:\n",
    "    if \"unique_id\" not in df.columns:\n",
    "        df = df.reset_index(names=\"unique_id\")\n",
    "\n",
    "    values_col = values_col if values_col else df.columns[-1]\n",
    "    df = pd.pivot(df, columns=\"unique_id\", index=\"ds\", values=values_col)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b78fc-6204-4c46-81b7-ab9df1e076df",
   "metadata": {},
   "source": [
    "### Baselines (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82449869-eefe-4506-8191-c1ba2b37f7d6",
   "metadata": {},
   "source": [
    "Implement baselines for neural models:\n",
    "- last value (naive)\n",
    "- average\n",
    "- AutoARIMA\n",
    "- AutoETS (with damped trend)\n",
    "\n",
    "Each dataset is worth 1 point. `sktime` will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f3e85-6925-4bdb-9fd6-25dce7cc2f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e49d112-b931-4645-abf7-dc12f47314c5",
   "metadata": {},
   "source": [
    "### Linear models (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d6227-3bf0-4783-a3c8-4e51006585d6",
   "metadata": {},
   "source": [
    "Implement linear neural models:\n",
    "- multioutput linear regression\n",
    "- LTSF Linear\n",
    "- LTSF DLinear\n",
    "- LTSF NLinear\n",
    "\n",
    "Note that Linear is a multi-channel model, while multioutput linear regression is single-channel.\n",
    "\n",
    "Tune the lookback window, the only hyperparameter of those models, or justify your choice in a comment if you don't. You can check the papers for reasonable values.\n",
    "\n",
    "If you use a given model, train it on both datasets. Each model is worth 0.5 points. Useful libraries: `sktime`, `neuralforecast`, PyTorch.\n",
    "\n",
    "Useful references:\n",
    "- [\"Are Transformers Effective for Time Series Forecasting?\" A. Zeng et al.](https://arxiv.org/abs/2205.13504) ([GitHub code](https://github.com/cure-lab/LTSF-Linear))\n",
    "- [\"An Analysis of Linear Time Series Forecasting Models\" W.Toner, L. Darlow](https://arxiv.org/abs/2403.14587) ([GitHub code](https://github.com/sir-lab/linear-forecasting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b5683-ea29-48b0-bbe2-db86982b8142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe2568b8-117e-456b-bfa7-8050c4506378",
   "metadata": {},
   "source": [
    "### MLP-based models (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3c39c6-e072-418f-ae1a-85db30f3c871",
   "metadata": {},
   "source": [
    "Implement MLP-based neural models:\n",
    "- N-BEATS\n",
    "- TSMixer\n",
    "\n",
    "For N-BEATS, use the interpretable architecture variant. If you want to tune hyperparameters, you can use e.g. automated class from `neuralforecast` with Ray or Optuna frameworks.\n",
    "\n",
    "Training each model on each dataset is worth 0.5 points. Useful libraries: `neuralforecast`, PyTorch, `pytorch-tsmixer`.\n",
    "\n",
    "Other interesting MLP-based models are e.g. N-HiTS, TiDE, TimeMixer, SOFTS. Each additional model is graded like models above.\n",
    "\n",
    "Useful references:\n",
    "- [\"N-BEATS: Neural basis expansion analysis for interpretable time series forecasting\" B. Oreshkin et al.](https://arxiv.org/abs/1905.10437)\n",
    "- [\"TSMixer: An All-MLP Architecture for Time Series Forecasting\" S. Chen et al.](https://arxiv.org/abs/2303.06053)\n",
    "- [\"N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting\" C. Challu et al.](https://arxiv.org/abs/2201.12886)\n",
    "- [\"Long-term Forecasting with TiDE: Time-series Dense Encoder\" A. Das et al.](https://arxiv.org/abs/2304.08424)\n",
    "- [\"TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting\" S. Wang et al.](https://arxiv.org/abs/2405.14616)\n",
    "- [\"SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion\" L. Han et al.](https://arxiv.org/abs/2404.14197)\n",
    "- [neuralforecast forecasting models list](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12557bfa-b122-4ff3-8d86-90d7544bcbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2847724c-9658-42c6-b7bb-f264f429231f",
   "metadata": {},
   "source": [
    "### Time series transformers (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c301a2c2-728c-45b2-bebb-0ab76ac6a6ee",
   "metadata": {},
   "source": [
    "Implement a time series transformer, e.g. PatchTST.\n",
    "\n",
    "You can use either pretrained variant, or train from scratch. If you want to tune hyperparameters, you can use e.g. automated class from `neuralforecast` with Ray or Optuna frameworks.\n",
    "\n",
    "Training the model in any way is worth 2 points. You can also choose any other time series transformer, e.g. TFT, iTransformer, Autoformer. Useful libraries: `neuralforecast`, PyTorch, `transformers`, IBM Granite. Each model after the first one is worth 1 point. If you use PatchTST, using the pretrained one and training from scratch counts as two separate models.\n",
    "\n",
    "Useful references:\n",
    "- [\"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers\" Y. Nie et al.](https://arxiv.org/abs/2211.14730)\n",
    "- [\"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\" B. Lim et al.](https://arxiv.org/abs/1912.09363)\n",
    "- [\"iTransformer: Inverted Transformers Are Effective for Time Series Forecasting\" Y. Liu et al.](https://arxiv.org/abs/2310.06625)\n",
    "- [\"Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting\" H. Wu et al.](https://arxiv.org/abs/2106.13008)\n",
    "- [neuralforecast forecasting models list](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html)\n",
    "- [IBM Granite tutorial for pretrained PatchTST](https://github.com/ibm-granite/granite-tsfm/blob/main/notebooks/hfdemo/patch_tst_getting_started.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8dc59-56f8-4201-9256-471a97fafa99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0293c5c-2e33-426f-a814-695c811db10f",
   "metadata": {},
   "source": [
    "### Pretrained foundation models (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81206ff-0be4-48ba-a9fa-4a3f74b9d048",
   "metadata": {},
   "source": [
    "Use a pretrained time series foundation model for zero-shot forecasting.\n",
    "\n",
    "Examples are e.g. TimesFM, Lag-Llama, TimeGPT, Moirai. Model notes:\n",
    "1. TimesFM - using the PyTorch version of original library is suggested\n",
    "2. Lag-Llama - this is a probabilistic model, note that we are interested in point forecasts (mean probabilistic value)\n",
    "3. TimeGPT - as this is a proprietary model, you need to provide the API token, make sure you don't push it to a public repository!\n",
    "\n",
    "The first model is worth 2 points, and subsequent ones are worth 1 point each.\n",
    "\n",
    "Useful references:\n",
    "- [\"A decoder-only foundation model for time-series forecasting\" A. Das et al.](https://arxiv.org/abs/2310.10688)\n",
    "- [TimesFM repository](https://github.com/google-research/timesfm)\n",
    "- [\"Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting\" K. Rasul et al.](https://arxiv.org/abs/2310.08278)\n",
    "- [Lag-Llama repository](https://github.com/time-series-foundation-models/lag-llama)\n",
    "- [\"TimeGPT-1\" A. Garza et al.](https://arxiv.org/abs/2310.03589)\n",
    "- [TimeGPT docs](https://docs.nixtla.io/)\n",
    "- [\"Unified Training of Universal Time Series Forecasting Transformers\" G. Woo et al.](https://arxiv.org/abs/2402.02592)\n",
    "- [HuggingFace Moirai model page](https://huggingface.co/Salesforce/moirai-1.0-R-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcfd87-c749-46a5-931e-1ab8977b6fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
